{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pickle\n",
        "import requests\n",
        "import urllib.request\n",
        "import io\n",
        "import zipfile\n",
        "import warnings\n",
        "\n",
        "# html\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709617700351
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('data.zip', 'r') as zipper:\n",
        "      zipper.extractall()\n",
        "\n",
        "with open(os.path.join('.', 'train_val_data.pkl'), 'rb') as f:\n",
        "      train_data, val_data = pickle.load(f)\n",
        "\n",
        "with open(os.path.join('.','test_data.pkl'),'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "  # natural language and vocab\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "vocab = words.words()\n",
        "\n",
        "y_train = [label for url, html, label in train_data]\n",
        "y_val = [label for url, html, label in val_data]\n",
        "\n",
        "warnings.warn('Data loaded.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package words to /home/azureuser/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n/tmp/ipykernel_3965/3710231763.py:19: UserWarning: Data loaded.\n  warnings.warn('Data loaded.')\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709617722327
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "\n",
        "  # prepare data\n",
        "  def prepare_data(data, featurizer, is_train):\n",
        "      X = []\n",
        "      for index, datapoint in enumerate(data):\n",
        "          url, html, label = datapoint\n",
        "          html = html.lower()\n",
        "\n",
        "          features = featurizer(url, html)\n",
        "\n",
        "          # Gets the keys of the dictionary as descriptions, gets the values as the numerical features.\n",
        "          feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "          X.append(feature_values)\n",
        "\n",
        "      return X, feature_descriptions\n",
        "\n",
        "  # train model\n",
        "  def train_model(X_train, y_train):\n",
        "      model = LogisticRegression(solver='liblinear')\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      return model\n",
        "\n",
        "  # wrapper function for everything above\n",
        "  def instantiate_model(compiled_featurizer, train_data, val_data):\n",
        "      X_train, feature_descriptions = prepare_data(train_data, compiled_featurizer, True)\n",
        "      X_val, _ = prepare_data(val_data, compiled_featurizer, False)\n",
        "      X_test, feature_descriptions = prepare_data(test_data, compiled_featurizer, True)\n",
        "      Y_test =[label for url, html, label in test_data]\n",
        "      model = train_model(X_train, y_train)\n",
        "      evaluate_model(model,X_test,Y_test)\n",
        "      return model, X_train, X_val, feature_descriptions\n",
        "\n",
        "  # a wrapper function that takes in named a list of keyword argument functions\n",
        "  # each of those functions are given the URL and HTML and expected to return a list or dictionary with the appropriate features\n",
        "  def create_featurizer(**featurizers):\n",
        "      def featurizer(url, html):\n",
        "          features = {}\n",
        "\n",
        "          for group_name, featurizer in featurizers.items():\n",
        "              group_features = featurizer(url, html)\n",
        "\n",
        "              if type(group_features) == type([]):\n",
        "                  for feature_name, feature_value in zip(range(len(group_features)), group_features):\n",
        "                      features[group_name + ' [' + str(feature_name) + ']'] = feature_value\n",
        "              elif type(group_features) == type({}):\n",
        "                  for feature_name, feature_value in group_features.items():\n",
        "                      features[group_name + ' [' + feature_name + ']'] = feature_value\n",
        "              else:\n",
        "                  features[group_name] = feature_value\n",
        "\n",
        "          return features\n",
        "\n",
        "      return featurizer\n",
        "\n",
        "  # evaluate model\n",
        "  def evaluate_model(model, X_val, y_val):\n",
        "      y_val_pred = model.predict(X_val)\n",
        "\n",
        "      print(print_metrics(y_val, y_val_pred))\n",
        "      confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      return y_val_pred\n",
        "\n",
        "  # confusion matrices\n",
        "  import pandas as pd\n",
        "  import seaborn as sns\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  def plot_confusion_matrix(y_val, y_val_pred):\n",
        "      # Create the Confusion Matrix\n",
        "      cnf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      # Visualizing the Confusion Matrix\n",
        "      class_names = [0, 1]  # Our diagnosis categories\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      # Setting up and visualizing the plot (do not worry about the code below!)\n",
        "      tick_marks = np.arange(len(class_names))\n",
        "      plt.xticks(tick_marks, class_names)\n",
        "      plt.yticks(tick_marks, class_names)\n",
        "      sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='YlGnBu', fmt='g')  # Creating heatmap\n",
        "      ax.xaxis.set_label_position('top')\n",
        "      plt.tight_layout()\n",
        "      plt.title('Confusion matrix', y=1.1)\n",
        "      plt.ylabel('Actual Labels')\n",
        "      plt.xlabel('Predicted Labels')\n",
        "\n",
        "  # other metrics\n",
        "  def print_metrics(y_val, y_val_pred):\n",
        "      prf = precision_recall_fscore_support(y_val, y_val_pred)\n",
        "      return {'Accuracy': accuracy_score(y_val, y_val_pred), 'Precision': prf[0][1], 'Recall': prf[1][1],\n",
        "              'F-1 Score': prf[2][1]}\n",
        "\n",
        "  # gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword to lowercase).\n",
        "  def get_normalized_keyword_count(html, keyword):\n",
        "      # only concern words inside the body, to speed things up\n",
        "      try:\n",
        "          necessary_html = html.split('<body')[1].split('</body>')[0]\n",
        "      except:\n",
        "          necessary_html = html  # if it doesn't have a body...\n",
        "\n",
        "      return math.log(1 + necessary_html.count(keyword.lower()))  # log is a good normalizer\n",
        "\n",
        "  # count the number of words in a URL\n",
        "  def count_words_in_url(url):\n",
        "      for i in range(len(url), 2, -1):  # don't count the first letter, because sometimes that might be a word by itself\n",
        "          if url[:i].lower() in vocab:  # if it's a word\n",
        "              return 1 + count_words_in_url(url[i:])  # get more words, and keep counting\n",
        "      return 0  # no words in URL (or at least, it doesn't start with a word, such as NYTimes)\n",
        "\n",
        "  def url_extension_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      extensions = ['.com', '.org', '.edu', '.net', '.co', '.nz', '.media', '.za', '.fr', '.is', '.tv', '.press',\n",
        "                    '.news', '.uk', '.info', '.ca', '.agency', '.us', '.ru', '.su', '.biz', '.ir']\n",
        "\n",
        "      for extension in extensions:\n",
        "          features[extension] = url.endswith(extension)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def keyword_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      keywords = ['vertical', 'news', 'section', 'light', 'data', 'eq', 'medium', 'large', 'ad', 'header', 'text', 'js',\n",
        "                  'nav', 'analytics', 'article', 'menu', 'tv', 'cnn', 'button', 'icon', 'edition', 'span', 'item', 'label',\n",
        "                  'link', 'world', 'politics', 'president', 'donald', 'business', 'food', 'tech', 'style', 'amp', 'vr',\n",
        "                  'watch', 'search', 'list', 'media', 'wrapper', 'div', 'zn', 'card', 'var', 'prod', 'true', 'window', 'new',\n",
        "                  'color', 'width', 'container', 'mobile', 'fixed', 'flex', 'aria', 'tablet', 'desktop', 'type', 'size',\n",
        "                  'tracking', 'heading', 'logo', 'svg', 'path', 'fill', 'content', 'ul', 'li', 'shop', 'home', 'static',\n",
        "                  'wrap', 'main', 'img', 'celebrity', 'lazy', 'image', 'high', 'noscript', 'inner', 'margin', 'headline',\n",
        "                  'child', 'interest', 'john', 'movies', 'music', 'parents', 'real', 'warren', 'opens', 'share', 'people',\n",
        "                  'max', 'min', 'state', 'event', 'story', 'click', 'time', 'trump', 'elizabeth', 'year', 'visit', 'post',\n",
        "                  'public', 'module', 'latest', 'star', 'skip', 'imagesvc', 'posted', 'ltc', 'summer', 'square', 'solid',\n",
        "                  'default', 'super', 'house', 'pride', 'week', 'america', 'man', 'day', 'wp', 'york', 'id', 'gallery',\n",
        "                  'inside', 'calls', 'big', 'daughter', 'photo', 'joe', 'deal', 'app', 'special', 'source', 'red', 'table',\n",
        "                  'money', 'family', 'featured', 'makes', 'pete', 'michael', 'video', 'case', 'says', 'popup', 'carousel',\n",
        "                  'category', 'script', 'helvetica', 'feature', 'dark', 'extra', 'small', 'horizontal', 'bg', 'hierarchical',\n",
        "                  'paginated', 'siblings', 'grid', 'active', 'demand', 'background', 'height', 'cn', 'cd', 'src', 'cnnnext',\n",
        "                  'dam', 'report', 'trade', 'images', 'file', 'huawei', 'mueller', 'impeachment', 'retirement', 'tealium',\n",
        "                  'col', 'immigration', 'china', 'flag', 'track', 'tariffs', 'sanders', 'staff', 'fn', 'srcset', 'green',\n",
        "                  'orient', 'iran', 'morning', 'jun', 'debate', 'ocasio', 'cortez', 'voters', 'pelosi', 'barr', 'buttigieg',\n",
        "                  'american', 'object', 'javascript', 'uppercase', 'omtr', 'chris', 'dn', 'hfs', 'rachel', 'maddow', 'lh',\n",
        "                  'teasepicture', 'db', 'xl', 'articletitlesection', 'founders', 'mono', 'ttu', 'biden', 'boston', 'bold',\n",
        "                  'anglerfish', 'jeffrey', 'radius']\n",
        "\n",
        "      for keyword in keywords:\n",
        "          features[keyword] = get_normalized_keyword_count(html, keyword)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def url_word_count_featurizer(url, html):\n",
        "      return count_words_in_url(url.split('.')[-2])\n",
        "      # for example, www.google.com will return google and nytimes.com will return nytimes\n",
        "\n",
        "  compiled_featurizer = create_featurizer(\n",
        "      url_extension=url_extension_featurizer,\n",
        "      keyword=keyword_featurizer,\n",
        "      url_word_count=url_word_count_featurizer,\n",
        "      html_length=lambda url, html: len(html),\n",
        "      url_length=lambda url, html: len(url))\n",
        "\n",
        "  print('Beginning to train model.')\n",
        "  model, X_train, X_val, feature_descriptions = instantiate_model(compiled_featurizer, train_data, val_data)\n",
        "  print('Trained model.')\n",
        "    \n",
        "  return model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data\n",
        "\n",
        "model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data = load()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package words to /home/azureuser/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Beginning to train model.\n{'Accuracy': 0.8252032520325203, 'Precision': 0.7225806451612903, 'Recall': 1.0, 'F-1 Score': 0.8389513108614232}\nTrained model.\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1709542314108
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "  # prepare data\n",
        "  def prepare_data(data, featurizer, is_train):\n",
        "      X = []\n",
        "      for index, datapoint in enumerate(data):\n",
        "          url, html, label = datapoint\n",
        "          html = html.lower()\n",
        "\n",
        "          features = featurizer(url, html)\n",
        "\n",
        "          # Gets the keys of the dictionary as descriptions, gets the values as the numerical features.\n",
        "          feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "          X.append(feature_values)\n",
        "\n",
        "      return X, feature_descriptions\n",
        "\n",
        "  # train model\n",
        "  def train_model(X_train, y_train):\n",
        "      model = LogisticRegression(multi_class='ovr' ,solver='sag')\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      return model\n",
        "\n",
        "  # wrapper function for everything above\n",
        "  def instantiate_model(compiled_featurizer, train_data, val_data):\n",
        "      X_train, feature_descriptions = prepare_data(train_data, compiled_featurizer, True)\n",
        "      X_val, _ = prepare_data(val_data, compiled_featurizer, False)\n",
        "      X_test, feature_descriptions = prepare_data(test_data, compiled_featurizer, True)\n",
        "      Y_test =[label for url, html, label in test_data]\n",
        "      model = train_model(X_train, y_train)\n",
        "      evaluate_model(model,X_test,Y_test)\n",
        "      return model, X_train, X_val, feature_descriptions\n",
        "\n",
        "  # a wrapper function that takes in named a list of keyword argument functions\n",
        "  # each of those functions are given the URL and HTML and expected to return a list or dictionary with the appropriate features\n",
        "  def create_featurizer(**featurizers):\n",
        "      def featurizer(url, html):\n",
        "          features = {}\n",
        "\n",
        "          for group_name, featurizer in featurizers.items():\n",
        "              group_features = featurizer(url, html)\n",
        "\n",
        "              if type(group_features) == type([]):\n",
        "                  for feature_name, feature_value in zip(range(len(group_features)), group_features):\n",
        "                      features[group_name + ' [' + str(feature_name) + ']'] = feature_value\n",
        "              elif type(group_features) == type({}):\n",
        "                  for feature_name, feature_value in group_features.items():\n",
        "                      features[group_name + ' [' + feature_name + ']'] = feature_value\n",
        "              else:\n",
        "                  features[group_name] = feature_value\n",
        "\n",
        "          return features\n",
        "\n",
        "      return featurizer\n",
        "\n",
        "  # evaluate model\n",
        "  def evaluate_model(model, X_val, y_val):\n",
        "      y_val_pred = model.predict(X_val)\n",
        "\n",
        "      print(print_metrics(y_val, y_val_pred))\n",
        "      confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      return y_val_pred\n",
        "\n",
        "  # confusion matrices\n",
        "  import pandas as pd\n",
        "  import seaborn as sns\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  def plot_confusion_matrix(y_val, y_val_pred):\n",
        "      # Create the Confusion Matrix\n",
        "      cnf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      # Visualizing the Confusion Matrix\n",
        "      class_names = [0, 1]  # Our diagnosis categories\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      # Setting up and visualizing the plot (do not worry about the code below!)\n",
        "      tick_marks = np.arange(len(class_names))\n",
        "      plt.xticks(tick_marks, class_names)\n",
        "      plt.yticks(tick_marks, class_names)\n",
        "      sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='YlGnBu', fmt='g')  # Creating heatmap\n",
        "      ax.xaxis.set_label_position('top')\n",
        "      plt.tight_layout()\n",
        "      plt.title('Confusion matrix', y=1.1)\n",
        "      plt.ylabel('Actual Labels')\n",
        "      plt.xlabel('Predicted Labels')\n",
        "\n",
        "  # other metrics\n",
        "  def print_metrics(y_val, y_val_pred):\n",
        "      prf = precision_recall_fscore_support(y_val, y_val_pred)\n",
        "      return {'Accuracy': accuracy_score(y_val, y_val_pred), 'Precision': prf[0][1], 'Recall': prf[1][1],\n",
        "              'F-1 Score': prf[2][1]}\n",
        "\n",
        "  # gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword to lowercase).\n",
        "  def get_normalized_keyword_count(html, keyword):\n",
        "      # only concern words inside the body, to speed things up\n",
        "      try:\n",
        "          necessary_html = html.split('<body')[1].split('</body>')[0]\n",
        "      except:\n",
        "          necessary_html = html  # if it doesn't have a body...\n",
        "\n",
        "      return math.log(1 + necessary_html.count(keyword.lower()))  # log is a good normalizer\n",
        "\n",
        "  # count the number of words in a URL\n",
        "  def count_words_in_url(url):\n",
        "      for i in range(len(url), 2, -1):  # don't count the first letter, because sometimes that might be a word by itself\n",
        "          if url[:i].lower() in vocab:  # if it's a word\n",
        "              return 1 + count_words_in_url(url[i:])  # get more words, and keep counting\n",
        "      return 0  # no words in URL (or at least, it doesn't start with a word, such as NYTimes)\n",
        "\n",
        "  def url_extension_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      extensions = ['.com', '.org', '.edu', '.net', '.co', '.nz', '.media', '.za', '.fr', '.is', '.tv', '.press',\n",
        "                    '.news', '.uk', '.info', '.ca', '.agency', '.us', '.ru', '.su', '.biz', '.ir']\n",
        "\n",
        "      for extension in extensions:\n",
        "          features[extension] = url.endswith(extension)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def keyword_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      keywords = ['vertical', 'news', 'section', 'light', 'data', 'eq', 'medium', 'large', 'ad', 'header', 'text', 'js',\n",
        "                  'nav', 'analytics', 'article', 'menu', 'tv', 'cnn', 'button', 'icon', 'edition', 'span', 'item', 'label',\n",
        "                  'link', 'world', 'politics', 'president', 'donald', 'business', 'food', 'tech', 'style', 'amp', 'vr',\n",
        "                  'watch', 'search', 'list', 'media', 'wrapper', 'div', 'zn', 'card', 'var', 'prod', 'true', 'window', 'new',\n",
        "                  'color', 'width', 'container', 'mobile', 'fixed', 'flex', 'aria', 'tablet', 'desktop', 'type', 'size',\n",
        "                  'tracking', 'heading', 'logo', 'svg', 'path', 'fill', 'content', 'ul', 'li', 'shop', 'home', 'static',\n",
        "                  'wrap', 'main', 'img', 'celebrity', 'lazy', 'image', 'high', 'noscript', 'inner', 'margin', 'headline',\n",
        "                  'child', 'interest', 'john', 'movies', 'music', 'parents', 'real', 'warren', 'opens', 'share', 'people',\n",
        "                  'max', 'min', 'state', 'event', 'story', 'click', 'time', 'trump', 'elizabeth', 'year', 'visit', 'post',\n",
        "                  'public', 'module', 'latest', 'star', 'skip', 'imagesvc', 'posted', 'ltc', 'summer', 'square', 'solid',\n",
        "                  'default', 'super', 'house', 'pride', 'week', 'america', 'man', 'day', 'wp', 'york', 'id', 'gallery',\n",
        "                  'inside', 'calls', 'big', 'daughter', 'photo', 'joe', 'deal', 'app', 'special', 'source', 'red', 'table',\n",
        "                  'money', 'family', 'featured', 'makes', 'pete', 'michael', 'video', 'case', 'says', 'popup', 'carousel',\n",
        "                  'category', 'script', 'helvetica', 'feature', 'dark', 'extra', 'small', 'horizontal', 'bg', 'hierarchical',\n",
        "                  'paginated', 'siblings', 'grid', 'active', 'demand', 'background', 'height', 'cn', 'cd', 'src', 'cnnnext',\n",
        "                  'dam', 'report', 'trade', 'images', 'file', 'huawei', 'mueller', 'impeachment', 'retirement', 'tealium',\n",
        "                  'col', 'immigration', 'china', 'flag', 'track', 'tariffs', 'sanders', 'staff', 'fn', 'srcset', 'green',\n",
        "                  'orient', 'iran', 'morning', 'jun', 'debate', 'ocasio', 'cortez', 'voters', 'pelosi', 'barr', 'buttigieg',\n",
        "                  'american', 'object', 'javascript', 'uppercase', 'omtr', 'chris', 'dn', 'hfs', 'rachel', 'maddow', 'lh',\n",
        "                  'teasepicture', 'db', 'xl', 'articletitlesection', 'founders', 'mono', 'ttu', 'biden', 'boston', 'bold',\n",
        "                  'anglerfish', 'jeffrey', 'radius']\n",
        "\n",
        "      for keyword in keywords:\n",
        "          features[keyword] = get_normalized_keyword_count(html, keyword)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def url_word_count_featurizer(url, html):\n",
        "      return count_words_in_url(url.split('.')[-2])\n",
        "      # for example, www.google.com will return google and nytimes.com will return nytimes\n",
        "\n",
        "  compiled_featurizer = create_featurizer(\n",
        "      url_extension=url_extension_featurizer,\n",
        "      keyword=keyword_featurizer,\n",
        "      url_word_count=url_word_count_featurizer,\n",
        "      html_length=lambda url, html: len(html),\n",
        "      url_length=lambda url, html: len(url))\n",
        "\n",
        "  print('Beginning to train model.')\n",
        "  model, X_train, X_val, feature_descriptions = instantiate_model(compiled_featurizer, train_data, val_data)\n",
        "  print('Trained model.')\n",
        "    \n",
        "  return model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data\n",
        "\n",
        "model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data = load()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Beginning to train model.\n{'Accuracy': 0.8252032520325203, 'Precision': 0.7225806451612903, 'Recall': 1.0, 'F-1 Score': 0.8389513108614232}\nTrained model.\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\"The max_iter was reached which means \"\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709617966467
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "  # prepare data\n",
        "  def prepare_data(data, featurizer, is_train):\n",
        "      X = []\n",
        "      for index, datapoint in enumerate(data):\n",
        "          url, html, label = datapoint\n",
        "          html = html.lower()\n",
        "\n",
        "          features = featurizer(url, html)\n",
        "\n",
        "          # Gets the keys of the dictionary as descriptions, gets the values as the numerical features.\n",
        "          feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "          X.append(feature_values)\n",
        "\n",
        "      return X, feature_descriptions\n",
        "\n",
        "  # train model\n",
        "  def train_model(X_train, y_train):\n",
        "      model = LogisticRegression(multi_class='ovr' ,solver='liblinear',n_jobs=9)\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      return model\n",
        "\n",
        "  # wrapper function for everything above\n",
        "  def instantiate_model(compiled_featurizer, train_data, val_data):\n",
        "      X_train, feature_descriptions = prepare_data(train_data, compiled_featurizer, True)\n",
        "      X_val, _ = prepare_data(val_data, compiled_featurizer, False)\n",
        "      X_test, feature_descriptions = prepare_data(test_data, compiled_featurizer, True)\n",
        "      Y_test =[label for url, html, label in test_data]\n",
        "      model = train_model(X_train, y_train)\n",
        "      evaluate_model(model,X_test,Y_test)\n",
        "      return model, X_train, X_val, feature_descriptions\n",
        "\n",
        "  # a wrapper function that takes in named a list of keyword argument functions\n",
        "  # each of those functions are given the URL and HTML and expected to return a list or dictionary with the appropriate features\n",
        "  def create_featurizer(**featurizers):\n",
        "      def featurizer(url, html):\n",
        "          features = {}\n",
        "\n",
        "          for group_name, featurizer in featurizers.items():\n",
        "              group_features = featurizer(url, html)\n",
        "\n",
        "              if type(group_features) == type([]):\n",
        "                  for feature_name, feature_value in zip(range(len(group_features)), group_features):\n",
        "                      features[group_name + ' [' + str(feature_name) + ']'] = feature_value\n",
        "              elif type(group_features) == type({}):\n",
        "                  for feature_name, feature_value in group_features.items():\n",
        "                      features[group_name + ' [' + feature_name + ']'] = feature_value\n",
        "              else:\n",
        "                  features[group_name] = feature_value\n",
        "\n",
        "          return features\n",
        "\n",
        "      return featurizer\n",
        "\n",
        "  # evaluate model\n",
        "  def evaluate_model(model, X_val, y_val):\n",
        "      y_val_pred = model.predict(X_val)\n",
        "\n",
        "      print(print_metrics(y_val, y_val_pred))\n",
        "      confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      return y_val_pred\n",
        "\n",
        "  # confusion matrices\n",
        "  import pandas as pd\n",
        "  import seaborn as sns\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  def plot_confusion_matrix(y_val, y_val_pred):\n",
        "      # Create the Confusion Matrix\n",
        "      cnf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      # Visualizing the Confusion Matrix\n",
        "      class_names = [0, 1]  # Our diagnosis categories\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      # Setting up and visualizing the plot (do not worry about the code below!)\n",
        "      tick_marks = np.arange(len(class_names))\n",
        "      plt.xticks(tick_marks, class_names)\n",
        "      plt.yticks(tick_marks, class_names)\n",
        "      sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='YlGnBu', fmt='g')  # Creating heatmap\n",
        "      ax.xaxis.set_label_position('top')\n",
        "      plt.tight_layout()\n",
        "      plt.title('Confusion matrix', y=1.1)\n",
        "      plt.ylabel('Actual Labels')\n",
        "      plt.xlabel('Predicted Labels')\n",
        "\n",
        "  # other metrics\n",
        "  def print_metrics(y_val, y_val_pred):\n",
        "      prf = precision_recall_fscore_support(y_val, y_val_pred)\n",
        "      return {'Accuracy': accuracy_score(y_val, y_val_pred), 'Precision': prf[0][1], 'Recall': prf[1][1],\n",
        "              'F-1 Score': prf[2][1]}\n",
        "\n",
        "  # gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword to lowercase).\n",
        "  def get_normalized_keyword_count(html, keyword):\n",
        "      # only concern words inside the body, to speed things up\n",
        "      try:\n",
        "          necessary_html = html.split('<body')[1].split('</body>')[0]\n",
        "      except:\n",
        "          necessary_html = html  # if it doesn't have a body...\n",
        "\n",
        "      return math.log(1 + necessary_html.count(keyword.lower()))  # log is a good normalizer\n",
        "\n",
        "  # count the number of words in a URL\n",
        "  def count_words_in_url(url):\n",
        "      for i in range(len(url), 2, -1):  # don't count the first letter, because sometimes that might be a word by itself\n",
        "          if url[:i].lower() in vocab:  # if it's a word\n",
        "              return 1 + count_words_in_url(url[i:])  # get more words, and keep counting\n",
        "      return 0  # no words in URL (or at least, it doesn't start with a word, such as NYTimes)\n",
        "\n",
        "  def url_extension_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      extensions = ['.com', '.org', '.edu', '.net', '.co', '.nz', '.media', '.za', '.fr', '.is', '.tv', '.press',\n",
        "                    '.news', '.uk', '.info', '.ca', '.agency', '.us', '.ru', '.su', '.biz', '.ir']\n",
        "\n",
        "      for extension in extensions:\n",
        "          features[extension] = url.endswith(extension)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def keyword_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      keywords = ['vertical', 'news', 'section', 'light', 'data', 'eq', 'medium', 'large', 'ad', 'header', 'text', 'js',\n",
        "                  'nav', 'analytics', 'article', 'menu', 'tv', 'cnn', 'button', 'icon', 'edition', 'span', 'item', 'label',\n",
        "                  'link', 'world', 'politics', 'president', 'donald', 'business', 'food', 'tech', 'style', 'amp', 'vr',\n",
        "                  'watch', 'search', 'list', 'media', 'wrapper', 'div', 'zn', 'card', 'var', 'prod', 'true', 'window', 'new',\n",
        "                  'color', 'width', 'container', 'mobile', 'fixed', 'flex', 'aria', 'tablet', 'desktop', 'type', 'size',\n",
        "                  'tracking', 'heading', 'logo', 'svg', 'path', 'fill', 'content', 'ul', 'li', 'shop', 'home', 'static',\n",
        "                  'wrap', 'main', 'img', 'celebrity', 'lazy', 'image', 'high', 'noscript', 'inner', 'margin', 'headline',\n",
        "                  'child', 'interest', 'john', 'movies', 'music', 'parents', 'real', 'warren', 'opens', 'share', 'people',\n",
        "                  'max', 'min', 'state', 'event', 'story', 'click', 'time', 'trump', 'elizabeth', 'year', 'visit', 'post',\n",
        "                  'public', 'module', 'latest', 'star', 'skip', 'imagesvc', 'posted', 'ltc', 'summer', 'square', 'solid',\n",
        "                  'default', 'super', 'house', 'pride', 'week', 'america', 'man', 'day', 'wp', 'york', 'id', 'gallery',\n",
        "                  'inside', 'calls', 'big', 'daughter', 'photo', 'joe', 'deal', 'app', 'special', 'source', 'red', 'table',\n",
        "                  'money', 'family', 'featured', 'makes', 'pete', 'michael', 'video', 'case', 'says', 'popup', 'carousel',\n",
        "                  'category', 'script', 'helvetica', 'feature', 'dark', 'extra', 'small', 'horizontal', 'bg', 'hierarchical',\n",
        "                  'paginated', 'siblings', 'grid', 'active', 'demand', 'background', 'height', 'cn', 'cd', 'src', 'cnnnext',\n",
        "                  'dam', 'report', 'trade', 'images', 'file', 'huawei', 'mueller', 'impeachment', 'retirement', 'tealium',\n",
        "                  'col', 'immigration', 'china', 'flag', 'track', 'tariffs', 'sanders', 'staff', 'fn', 'srcset', 'green',\n",
        "                  'orient', 'iran', 'morning', 'jun', 'debate', 'ocasio', 'cortez', 'voters', 'pelosi', 'barr', 'buttigieg',\n",
        "                  'american', 'object', 'javascript', 'uppercase', 'omtr', 'chris', 'dn', 'hfs', 'rachel', 'maddow', 'lh',\n",
        "                  'teasepicture', 'db', 'xl', 'articletitlesection', 'founders', 'mono', 'ttu', 'biden', 'boston', 'bold',\n",
        "                  'anglerfish', 'jeffrey', 'radius']\n",
        "\n",
        "      for keyword in keywords:\n",
        "          features[keyword] = get_normalized_keyword_count(html, keyword)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def url_word_count_featurizer(url, html):\n",
        "      return count_words_in_url(url.split('.')[-2])\n",
        "      # for example, www.google.com will return google and nytimes.com will return nytimes\n",
        "\n",
        "  compiled_featurizer = create_featurizer(\n",
        "      url_extension=url_extension_featurizer,\n",
        "      keyword=keyword_featurizer,\n",
        "      url_word_count=url_word_count_featurizer,\n",
        "      html_length=lambda url, html: len(html),\n",
        "      url_length=lambda url, html: len(url))\n",
        "\n",
        "  print('Beginning to train model.')\n",
        "  model, X_train, X_val, feature_descriptions = instantiate_model(compiled_featurizer, train_data, val_data)\n",
        "  print('Trained model.')\n",
        "    \n",
        "  return model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data\n",
        "\n",
        "model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data = load()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Beginning to train model.\n{'Accuracy': 0.8252032520325203, 'Precision': 0.7225806451612903, 'Recall': 1.0, 'F-1 Score': 0.8389513108614232}\nTrained model.\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1537: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 9.\n  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709618210434
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "  # prepare data\n",
        "  def prepare_data(data, featurizer, is_train):\n",
        "      X = []\n",
        "      for index, datapoint in enumerate(data):\n",
        "          url, html, label = datapoint\n",
        "          html = html.lower()\n",
        "\n",
        "          features = featurizer(url, html)\n",
        "\n",
        "          # Gets the keys of the dictionary as descriptions, gets the values as the numerical features.\n",
        "          feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "          X.append(feature_values)\n",
        "\n",
        "      return X, feature_descriptions\n",
        "\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  # train model\n",
        "  def train_model(X_train, y_train):\n",
        "      model = GaussianNB()\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      return model\n",
        "\n",
        "  # wrapper function for everything above\n",
        "  def instantiate_model(compiled_featurizer, train_data, val_data):\n",
        "      X_train, feature_descriptions = prepare_data(train_data, compiled_featurizer, True)\n",
        "      X_val, _ = prepare_data(val_data, compiled_featurizer, False)\n",
        "      X_test, feature_descriptions = prepare_data(test_data, compiled_featurizer, True)\n",
        "      Y_test =[label for url, html, label in test_data]\n",
        "      model = train_model(X_train, y_train)\n",
        "      evaluate_model(model,X_test,Y_test)\n",
        "      return model, X_train, X_val, feature_descriptions\n",
        "\n",
        "  # a wrapper function that takes in named a list of keyword argument functions\n",
        "  # each of those functions are given the URL and HTML and expected to return a list or dictionary with the appropriate features\n",
        "  def create_featurizer(**featurizers):\n",
        "      def featurizer(url, html):\n",
        "          features = {}\n",
        "\n",
        "          for group_name, featurizer in featurizers.items():\n",
        "              group_features = featurizer(url, html)\n",
        "\n",
        "              if type(group_features) == type([]):\n",
        "                  for feature_name, feature_value in zip(range(len(group_features)), group_features):\n",
        "                      features[group_name + ' [' + str(feature_name) + ']'] = feature_value\n",
        "              elif type(group_features) == type({}):\n",
        "                  for feature_name, feature_value in group_features.items():\n",
        "                      features[group_name + ' [' + feature_name + ']'] = feature_value\n",
        "              else:\n",
        "                  features[group_name] = feature_value\n",
        "\n",
        "          return features\n",
        "\n",
        "      return featurizer\n",
        "\n",
        "  # evaluate model\n",
        "  def evaluate_model(model, X_val, y_val):\n",
        "      y_val_pred = model.predict(X_val)\n",
        "\n",
        "      print(print_metrics(y_val, y_val_pred))\n",
        "      confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      return y_val_pred\n",
        "\n",
        "  # confusion matrices\n",
        "  import pandas as pd\n",
        "  import seaborn as sns\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  def plot_confusion_matrix(y_val, y_val_pred):\n",
        "      # Create the Confusion Matrix\n",
        "      cnf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      # Visualizing the Confusion Matrix\n",
        "      class_names = [0, 1]  # Our diagnosis categories\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      # Setting up and visualizing the plot (do not worry about the code below!)\n",
        "      tick_marks = np.arange(len(class_names))\n",
        "      plt.xticks(tick_marks, class_names)\n",
        "      plt.yticks(tick_marks, class_names)\n",
        "      sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='YlGnBu', fmt='g')  # Creating heatmap\n",
        "      ax.xaxis.set_label_position('top')\n",
        "      plt.tight_layout()\n",
        "      plt.title('Confusion matrix', y=1.1)\n",
        "      plt.ylabel('Actual Labels')\n",
        "      plt.xlabel('Predicted Labels')\n",
        "\n",
        "  # other metrics\n",
        "  def print_metrics(y_val, y_val_pred):\n",
        "      prf = precision_recall_fscore_support(y_val, y_val_pred)\n",
        "      return {'Accuracy': accuracy_score(y_val, y_val_pred), 'Precision': prf[0][1], 'Recall': prf[1][1],\n",
        "              'F-1 Score': prf[2][1]}\n",
        "\n",
        "  # gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword to lowercase).\n",
        "  def get_normalized_keyword_count(html, keyword):\n",
        "      # only concern words inside the body, to speed things up\n",
        "      try:\n",
        "          necessary_html = html.split('<body')[1].split('</body>')[0]\n",
        "      except:\n",
        "          necessary_html = html  # if it doesn't have a body...\n",
        "\n",
        "      return math.log(1 + necessary_html.count(keyword.lower()))  # log is a good normalizer\n",
        "\n",
        "  # count the number of words in a URL\n",
        "  def count_words_in_url(url):\n",
        "      for i in range(len(url), 2, -1):  # don't count the first letter, because sometimes that might be a word by itself\n",
        "          if url[:i].lower() in vocab:  # if it's a word\n",
        "              return 1 + count_words_in_url(url[i:])  # get more words, and keep counting\n",
        "      return 0  # no words in URL (or at least, it doesn't start with a word, such as NYTimes)\n",
        "\n",
        "  def url_extension_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      extensions = ['.com', '.org', '.edu', '.net', '.co', '.nz', '.media', '.za', '.fr', '.is', '.tv', '.press',\n",
        "                    '.news', '.uk', '.info', '.ca', '.agency', '.us', '.ru', '.su', '.biz', '.ir']\n",
        "\n",
        "      for extension in extensions:\n",
        "          features[extension] = url.endswith(extension)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def keyword_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      keywords = ['vertical', 'news', 'section', 'light', 'data', 'eq', 'medium', 'large', 'ad', 'header', 'text', 'js',\n",
        "                  'nav', 'analytics', 'article', 'menu', 'tv', 'cnn', 'button', 'icon', 'edition', 'span', 'item', 'label',\n",
        "                  'link', 'world', 'politics', 'president', 'donald', 'business', 'food', 'tech', 'style', 'amp', 'vr',\n",
        "                  'watch', 'search', 'list', 'media', 'wrapper', 'div', 'zn', 'card', 'var', 'prod', 'true', 'window', 'new',\n",
        "                  'color', 'width', 'container', 'mobile', 'fixed', 'flex', 'aria', 'tablet', 'desktop', 'type', 'size',\n",
        "                  'tracking', 'heading', 'logo', 'svg', 'path', 'fill', 'content', 'ul', 'li', 'shop', 'home', 'static',\n",
        "                  'wrap', 'main', 'img', 'celebrity', 'lazy', 'image', 'high', 'noscript', 'inner', 'margin', 'headline',\n",
        "                  'child', 'interest', 'john', 'movies', 'music', 'parents', 'real', 'warren', 'opens', 'share', 'people',\n",
        "                  'max', 'min', 'state', 'event', 'story', 'click', 'time', 'trump', 'elizabeth', 'year', 'visit', 'post',\n",
        "                  'public', 'module', 'latest', 'star', 'skip', 'imagesvc', 'posted', 'ltc', 'summer', 'square', 'solid',\n",
        "                  'default', 'super', 'house', 'pride', 'week', 'america', 'man', 'day', 'wp', 'york', 'id', 'gallery',\n",
        "                  'inside', 'calls', 'big', 'daughter', 'photo', 'joe', 'deal', 'app', 'special', 'source', 'red', 'table',\n",
        "                  'money', 'family', 'featured', 'makes', 'pete', 'michael', 'video', 'case', 'says', 'popup', 'carousel',\n",
        "                  'category', 'script', 'helvetica', 'feature', 'dark', 'extra', 'small', 'horizontal', 'bg', 'hierarchical',\n",
        "                  'paginated', 'siblings', 'grid', 'active', 'demand', 'background', 'height', 'cn', 'cd', 'src', 'cnnnext',\n",
        "                  'dam', 'report', 'trade', 'images', 'file', 'huawei', 'mueller', 'impeachment', 'retirement', 'tealium',\n",
        "                  'col', 'immigration', 'china', 'flag', 'track', 'tariffs', 'sanders', 'staff', 'fn', 'srcset', 'green',\n",
        "                  'orient', 'iran', 'morning', 'jun', 'debate', 'ocasio', 'cortez', 'voters', 'pelosi', 'barr', 'buttigieg',\n",
        "                  'american', 'object', 'javascript', 'uppercase', 'omtr', 'chris', 'dn', 'hfs', 'rachel', 'maddow', 'lh',\n",
        "                  'teasepicture', 'db', 'xl', 'articletitlesection', 'founders', 'mono', 'ttu', 'biden', 'boston', 'bold',\n",
        "                  'anglerfish', 'jeffrey', 'radius']\n",
        "\n",
        "      for keyword in keywords:\n",
        "          features[keyword] = get_normalized_keyword_count(html, keyword)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def url_word_count_featurizer(url, html):\n",
        "      return count_words_in_url(url.split('.')[-2])\n",
        "      # for example, www.google.com will return google and nytimes.com will return nytimes\n",
        "\n",
        "  compiled_featurizer = create_featurizer(\n",
        "      url_extension=url_extension_featurizer,\n",
        "      keyword=keyword_featurizer,\n",
        "      url_word_count=url_word_count_featurizer,\n",
        "      html_length=lambda url, html: len(html),\n",
        "      url_length=lambda url, html: len(url))\n",
        "\n",
        "  print('Beginning to train model.')\n",
        "  model, X_train, X_val, feature_descriptions = instantiate_model(compiled_featurizer, train_data, val_data)\n",
        "  print('Trained model.')\n",
        "    \n",
        "  return model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data\n",
        "\n",
        "model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data = load()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Beginning to train model.\n{'Accuracy': 0.6097560975609756, 'Precision': 0.5625, 'Recall': 0.6428571428571429, 'F-1 Score': 0.6000000000000001}\nTrained model.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709618454274
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "  # prepare data\n",
        "  def prepare_data(data, featurizer, is_train):\n",
        "      X = []\n",
        "      for index, datapoint in enumerate(data):\n",
        "          url, html, label = datapoint\n",
        "          html = html.lower()\n",
        "\n",
        "          features = featurizer(url, html)\n",
        "\n",
        "          # Gets the keys of the dictionary as descriptions, gets the values as the numerical features.\n",
        "          feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "          X.append(feature_values)\n",
        "\n",
        "      return X, feature_descriptions\n",
        "\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  # train model\n",
        "  def train_model(X_train, y_train):\n",
        "      model = GaussianNB()\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      return model\n",
        "\n",
        "  # wrapper function for everything above\n",
        "  def instantiate_model(compiled_featurizer, train_data, val_data):\n",
        "      X_train, feature_descriptions = prepare_data(train_data, compiled_featurizer, True)\n",
        "      X_val, _ = prepare_data(val_data, compiled_featurizer, False)\n",
        "      X_test, feature_descriptions = prepare_data(test_data, compiled_featurizer, True)\n",
        "      Y_test =[label for url, html, label in test_data]\n",
        "      model = train_model(X_train, y_train)\n",
        "      evaluate_model(model,X_test,Y_test)\n",
        "      return model, X_train, X_val, feature_descriptions\n",
        "\n",
        "  # a wrapper function that takes in named a list of keyword argument functions\n",
        "  # each of those functions are given the URL and HTML and expected to return a list or dictionary with the appropriate features\n",
        "  def create_featurizer(**featurizers):\n",
        "      def featurizer(url, html):\n",
        "          features = {}\n",
        "\n",
        "          for group_name, featurizer in featurizers.items():\n",
        "              group_features = featurizer(url, html)\n",
        "\n",
        "              if type(group_features) == type([]):\n",
        "                  for feature_name, feature_value in zip(range(len(group_features)), group_features):\n",
        "                      features[group_name + ' [' + str(feature_name) + ']'] = feature_value\n",
        "              elif type(group_features) == type({}):\n",
        "                  for feature_name, feature_value in group_features.items():\n",
        "                      features[group_name + ' [' + feature_name + ']'] = feature_value\n",
        "              else:\n",
        "                  features[group_name] = feature_value\n",
        "\n",
        "          return features\n",
        "\n",
        "      return featurizer\n",
        "\n",
        "  # evaluate model\n",
        "  def evaluate_model(model, X_val, y_val):\n",
        "      y_val_pred = model.predict(X_val)\n",
        "\n",
        "      print(print_metrics(y_val, y_val_pred))\n",
        "      confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      return y_val_pred\n",
        "\n",
        "  # confusion matrices\n",
        "  import pandas as pd\n",
        "  import seaborn as sns\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  def plot_confusion_matrix(y_val, y_val_pred):\n",
        "      # Create the Confusion Matrix\n",
        "      cnf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "      # Visualizing the Confusion Matrix\n",
        "      class_names = [0, 1]  # Our diagnosis categories\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      # Setting up and visualizing the plot (do not worry about the code below!)\n",
        "      tick_marks = np.arange(len(class_names))\n",
        "      plt.xticks(tick_marks, class_names)\n",
        "      plt.yticks(tick_marks, class_names)\n",
        "      sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap='YlGnBu', fmt='g')  # Creating heatmap\n",
        "      ax.xaxis.set_label_position('top')\n",
        "      plt.tight_layout()\n",
        "      plt.title('Confusion matrix', y=1.1)\n",
        "      plt.ylabel('Actual Labels')\n",
        "      plt.xlabel('Predicted Labels')\n",
        "\n",
        "  # other metrics\n",
        "  def print_metrics(y_val, y_val_pred):\n",
        "      prf = precision_recall_fscore_support(y_val, y_val_pred)\n",
        "      return {'Accuracy': accuracy_score(y_val, y_val_pred), 'Precision': prf[0][1], 'Recall': prf[1][1],\n",
        "              'F-1 Score': prf[2][1]}\n",
        "\n",
        "  # gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword to lowercase).\n",
        "  def get_normalized_keyword_count(html, keyword):\n",
        "      # only concern words inside the body, to speed things up\n",
        "      try:\n",
        "          necessary_html = html.split('<body')[1].split('</body>')[0]\n",
        "      except:\n",
        "          necessary_html = html  # if it doesn't have a body...\n",
        "\n",
        "      return math.log(1 + necessary_html.count(keyword.lower()))  # log is a good normalizer\n",
        "\n",
        "  # count the number of words in a URL\n",
        "  def count_words_in_url(url):\n",
        "      for i in range(len(url), 2, -1):  # don't count the first letter, because sometimes that might be a word by itself\n",
        "          if url[:i].lower() in vocab:  # if it's a word\n",
        "              return 1 + count_words_in_url(url[i:])  # get more words, and keep counting\n",
        "      return 0  # no words in URL (or at least, it doesn't start with a word, such as NYTimes)\n",
        "\n",
        "  def url_extension_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      extensions = ['.com', '.org', '.edu', '.net', '.co', '.nz', '.media', '.za', '.fr', '.is', '.tv', '.press',\n",
        "                    '.news', '.uk', '.info', '.ca', '.agency', '.us', '.ru', '.su', '.biz', '.ir']\n",
        "\n",
        "      for extension in extensions:\n",
        "          features[extension] = url.endswith(extension)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def keyword_featurizer(url, html):\n",
        "      features = {}\n",
        "\n",
        "      keywords = ['vertical', 'news', 'section', 'light', 'data', 'eq', 'medium', 'large', 'ad', 'header', 'text', 'js',\n",
        "                  'nav', 'analytics', 'article', 'menu', 'tv', 'cnn', 'button', 'icon', 'edition', 'span', 'item', 'label',\n",
        "                  'link', 'world', 'politics', 'president', 'donald', 'business', 'food', 'tech', 'style', 'amp', 'vr',\n",
        "                  'watch', 'search', 'list', 'media', 'wrapper', 'div', 'zn', 'card', 'var', 'prod', 'true', 'window', 'new',\n",
        "                  'color', 'width', 'container', 'mobile', 'fixed', 'flex', 'aria', 'tablet', 'desktop', 'type', 'size',\n",
        "                  'tracking', 'heading', 'logo', 'svg', 'path', 'fill', 'content', 'ul', 'li', 'shop', 'home', 'static',\n",
        "                  'wrap', 'main', 'img', 'celebrity', 'lazy', 'image', 'high', 'noscript', 'inner', 'margin', 'headline',\n",
        "                  'child', 'interest', 'john', 'movies', 'music', 'parents', 'real', 'warren', 'opens', 'share', 'people',\n",
        "                  'max', 'min', 'state', 'event', 'story', 'click', 'time', 'trump', 'elizabeth', 'year', 'visit', 'post',\n",
        "                  'public', 'module', 'latest', 'star', 'skip', 'imagesvc', 'posted', 'ltc', 'summer', 'square', 'solid',\n",
        "                  'default', 'super', 'house', 'pride', 'week', 'america', 'man', 'day', 'wp', 'york', 'id', 'gallery',\n",
        "                  'inside', 'calls', 'big', 'daughter', 'photo', 'joe', 'deal', 'app', 'special', 'source', 'red', 'table',\n",
        "                  'money', 'family', 'featured', 'makes', 'pete', 'michael', 'video', 'case', 'says', 'popup', 'carousel',\n",
        "                  'category', 'script', 'helvetica', 'feature', 'dark', 'extra', 'small', 'horizontal', 'bg', 'hierarchical',\n",
        "                  'paginated', 'siblings', 'grid', 'active', 'demand', 'background', 'height', 'cn', 'cd', 'src', 'cnnnext',\n",
        "                  'dam', 'report', 'trade', 'images', 'file', 'huawei', 'mueller', 'impeachment', 'retirement', 'tealium',\n",
        "                  'col', 'immigration', 'china', 'flag', 'track', 'tariffs', 'sanders', 'staff', 'fn', 'srcset', 'green',\n",
        "                  'orient', 'iran', 'morning', 'jun', 'debate', 'ocasio', 'cortez', 'voters', 'pelosi', 'barr', 'buttigieg',\n",
        "                  'american', 'object', 'javascript', 'uppercase', 'omtr', 'chris', 'dn', 'hfs', 'rachel', 'maddow', 'lh',\n",
        "                  'teasepicture', 'db', 'xl', 'articletitlesection', 'founders', 'mono', 'ttu', 'biden', 'boston', 'bold',\n",
        "                  'anglerfish', 'jeffrey', 'radius']\n",
        "\n",
        "      for keyword in keywords:\n",
        "          features[keyword] = get_normalized_keyword_count(html, keyword)\n",
        "\n",
        "      return features\n",
        "\n",
        "  def url_word_count_featurizer(url, html):\n",
        "      return count_words_in_url(url.split('.')[-2])\n",
        "      # for example, www.google.com will return google and nytimes.com will return nytimes\n",
        "\n",
        "  compiled_featurizer = create_featurizer(\n",
        "      url_extension=url_extension_featurizer,\n",
        "      keyword=keyword_featurizer,\n",
        "      url_word_count=url_word_count_featurizer,\n",
        "      html_length=lambda url, html: len(html),\n",
        "      url_length=lambda url, html: len(url))\n",
        "\n",
        "  print('Beginning to train model.')\n",
        "  model, X_train, X_val, feature_descriptions = instantiate_model(compiled_featurizer, train_data, val_data)\n",
        "  print('Trained model.')\n",
        "    \n",
        "  return model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data\n",
        "\n",
        "model, feature_descriptions, compiled_featurizer, requests, confusion_matrix, print_metrics, train_data, val_data = load()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Beginning to train model.\n{'Accuracy': 0.6097560975609756, 'Precision': 0.5625, 'Recall': 0.6428571428571429, 'F-1 Score': 0.6000000000000001}\nTrained model.\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709618698104
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}